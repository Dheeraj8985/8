import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Flatten,Dense

fashion_mnist=tf.keras.datasets.fashion_mnist

(training,train_label),(testing,test_label)=fashion_mnist.load_data()

training.shape

testing.shape

class_names=["Tshirt","trouser","pullover","dress","coat","sandal","shirt","sneaker","bag","ankelboot"]

#perform scaling of data from 0-255 to 0-1 range
training=training/255.0
testing=testing/255.0

plt.imshow(training[0])
plt.xlabel(train_label[0])

#check data plot 25 images
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(testing[i])
    plt.xlabel(class_names[test_label[i]])

# We begin by defining the a empty stack. We'll use this for building our 
# network, later by layer.
model =Sequential()

# We start with a convolutional layer this will extract features from 
# the input images by sliding a convolution filter over the input image, 
# resulting in a feature map.
model.add(
    tf.keras.layers.Conv2D(
        filters=32, # How many filters we will learn 
        kernel_size=(3, 3), # Size of feature map that will slide over image
        strides=(1, 1), # How the feature map "steps" across the image
        padding='valid', # We are not using padding
        activation='relu', # Rectified Linear Unit Activation Function
        input_shape=(28, 28, 1) # The expected input shape for this layer
    )
) 

# The next layer we will add is a Maxpooling layer. This will reduce the 
# dimensionality of each feature, which reduces the number of parameters that 
# the model needs to learn, which shortens training time.
model.add(
    tf.keras.layers.MaxPooling2D(
        pool_size=(2, 2), # Size feature will be mapped to
        strides=(2, 2) # How the pool "steps" across the feature
    )
)
          
# We'll now add a dropout layer. This fights overfitting and forces the model to 
# learn multiple representations of the same data by randomly disabling neurons 
# in the learning phase.
model.add(
    tf.keras.layers.Dropout(
        rate=0.25 # Randomly disable 25% of neurons
    )
)

# Output from previous layer is a 3D tensor. This must be flattened to a 1D 
# vector before beiung fed to the Dense Layers.
model.add(
    tf.keras.layers.Flatten()
)

# A dense (interconnected) layer is added for mapping the derived features 
# to the required class.
model.add(
    tf.keras.layers.Dense(
        units=128, # Output shape
        activation='relu' # Rectified Linear Unit Activation Function
    )
)

# Final layer with 10 outputs and a softmax activation. Softmax activation 
# enables me to calculate the output based on the probabilities. 
# Each class is assigned a probability and the class with the maximum 
# probability is the modelâ€™s output for the input.
model.add(
    tf.keras.layers.Dense(
        units=10, # Output shape
        activation='softmax' # Softmax Activation Function
    )
)

# Build the model
model.compile(
    loss=tf.keras.losses.sparse_categorical_crossentropy, # loss function
    optimizer=tf.keras.optimizers.Adam(), # optimizer function
    metrics=['accuracy'] # reporting metric
)

# Display a summary of the models structure
model.summary()

training = np.expand_dims(training, -1)
testing= np.expand_dims(testing, -1)
model.fit(training,train_label,epochs=10,batch_size=256,validation_split=0.2)

loss,accuracy=model.evaluate(testing,test_label)

accuracy

pred=model.predict(testing)

pred

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(testing[i])
    true_label=test_label[i]
    predicted_label=np.argmax(pred[i])
    if(true_label==predicted_label):
        color="green"
    else:
        color="red"
    plt.xlabel("{},{}".format(class_names[np.argmax(pred[i])],round(100*np.max(pred[i]),2)),color=color)
plt.show()